---
title: "assignment2"
date: "May 7, 2020"
output: html_document
---


##Introduction
The purpose of this assignment is t is to learn the main advantages and disadvantages of some machine learning regression methods by comparing:

* regression trees,
* bagging,
* random forest,
*  kernel approaches

</br>

##Data pre-processing

```{r, message=FALSE}
library(mice)
library(GGally)
library(geosphere)
library(dplyr)
library(VIM)
library(lubridate)
library(caTools)
library(e1071)
library(Metrics)
library(caret)
library(kernlab)



dataOriginal <- read.csv("Melbourne_housing_FULL.csv", header = TRUE, na.strings=c("", "NA", "#N/A"))

```


```{r, echo=FALSE}
# Fix date format
dataOriginal$Date<-as.Date(dataOriginal$Date, "%d/%m/%Y")
```

### NA values
Let's vizualise NA values
```{r}
aggr(dataOriginal,delimiter=NULL, plot=TRUE)
```

####Discard NA values of price
```{r}
dataOriginal <- dataOriginal[!is.na(dataOriginal$Price),]
```

####Replacing NA values

You can also embed plots, for example:

```{r}
#we don't need to do that apriori, but only if there will be any technique that doesn't work with NA 
set.seed(101) 
init = mice(dataOriginal, maxit=0) 
dataComplete <- complete(init)
```
</br>

###The target variable

####Distribution
```{r}
hist(dataOriginal$Price,col=4, main=paste("Price"), xlab="A$", breaks=seq(0,max(dataOriginal$Price),l=100))
```

#### Logarithmic distribution
```{r}
hist(log10(dataOriginal$Price),col=5, main=paste("Price"),xlab="log10(A$)", breaks=seq(log10(min(dataOriginal$Price)),log10(max(dataOriginal$Price)),l=100))
```

### Aggregating attributes and deriving new columns
```{r}
aggregetaData <- function(data) {
  
  
   # Add year column and street column
  addresses<-gsub("[[:punct:]]", "", data$Address)
  addresses<-gsub("[[:digit:]]", "", addresses)
  
  
  newData <- mutate(data, Street=addresses, YearSaled=year(data$Date))
  
  # Add season column
  newData <- mutate(newData, Season=ifelse(month(newData$Date) %in% 3:5, "Spring",ifelse(month(newData$Date) %in% 6:8, "summer", ifelse(month(newData$Date) %in% 9:11, "Autumn","Winter"))))
  
  # Remove unuseful columns
  newData<-select(newData,-c(Address,Date))
  
  # Add a column with the distance to the center of the bay
  MelbourneBayCentre <- cbind(144.783832, -38.104275)
  data <- mutate(data, DistanceToBayCentre = distGeo(cbind(Longtitude, Lattitude), MelbourneBayCentre)/1000) #in km
  
  # Make from string variables factor
  newData$Street <- factor(newData$Street)
  newData$Season <- factor(newData$Season)
  
  # Add a column with the distance to the airports
  MelbourneAirport<-c( 144.841802,-37.670149)
  AvalonAirport<- c( 144.473152,-38.027377)
  EssendonAirport<- c( 144.902035,-37.726376)
  MoorabbinAirport <- c( 145.100306,-37.979454)
  newData<-mutate(newData,DistanceToAirports = pmin(distGeo(cbind(Longtitude,Lattitude), MelbourneAirport), distGeo(cbind(Longtitude,Lattitude), AvalonAirport), distGeo(cbind(Longtitude,Lattitude), EssendonAirport), distGeo(cbind(Longtitude,Lattitude), MoorabbinAirport))/1000) #in km
  
  return(newData)
}

data <- aggregetaData(dataOriginal)
dataComplete <- aggregetaData(dataComplete)
```

</br>

#### Summary of data
```{r}
str(data)
summary(data)
```

</br>

####Small dataset
We created smaller sample of the data in purpose of the tuning of the hyperparameter of the model.
```{r}
set.seed(123)

split = sample.split(dataComplete, SplitRatio = 0.01)
smallDataset = subset(dataComplete, split == TRUE)

split = sample.split(data, SplitRatio = 0.7)
smallDatasetTrain = subset(smallDataset, split == TRUE)
smallDatasetTest = subset(smallDataset, split == FALSE)

traindDfWithoutPrice <- smallDatasetTrain[-4]
testDfWithoutPrice <- smallDatasetTest[-4]
```

</br>


</br>

##Support Vector Machine (SVM)
Although the method ksvm does not fail when given NA value, internally is omitting them and learning anything from them. So with SVM, we used the complete dataset created by data imputation.

### {.tabset .tabset-fade .tabset-pills}
Now we tried a couple of SVM with a different model on all columns of the data. We perform 10-fold cross-validation to see the training error and cross-validation error of each model.

#### Linear kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
linearSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="vanilladot", cross = 10, scaled = TRUE)
linearSVMclassifier
predictedPrice = predict(linearSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```

#### Anova kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
anovaSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="anovadot", cross = 10, scaled = TRUE)
anovaSVMclassifier
predictedPrice = predict(anovaSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Radial Basis kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
radialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="rbfdot", cross = 10, scaled = TRUE)
radialSVMclassifier
predictedPrice = predict(radialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 3 the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=3), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 5 {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=5), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 8 {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=8), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Laplacian kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
laplacianSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="laplacedot", cross = 10, scaled = TRUE)
laplacianSVMclassifier
predictedPrice = predict(laplacianSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```

#### Bessel kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
besselSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="besseldot", cross = 10, scaled = TRUE)
besselSVMclassifier
predictedPrice = predict(besselSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```

###Removing columns with too much NA values in original data
### {.tabset .tabset-fade .tabset-pills}
Now we try to remove columns which have many NA values. So we remove the columns Bedroom2, Car, YearBuild and lattitude and longtitide. Then we perform 10-fold cross validation to see the training error and cross validation error of each model on those reduced dataset.
```{r}
smallDatasetTrain <- smallDatasetTrain[-c(9,11,14,16,17)]
smallDatasetTest <- smallDatasetTest[-c(9,11,14,16,17)]
```

#### Linear kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
linearSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="vanilladot", cross = 10, scaled = TRUE)
linearSVMclassifier
predictedPrice = predict(linearSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```

#### Anova kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
anovaSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="anovadot", cross = 10, scaled = TRUE)
anovaSVMclassifier
predictedPrice = predict(anovaSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Radial Basis kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
radialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="rbfdot", cross = 10, scaled = TRUE)
radialSVMclassifier
predictedPrice = predict(radialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 3 the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=3), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 5 {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=5), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 8 {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=8), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Laplacian kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
laplacianSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="laplacedot", cross = 10, scaled = TRUE)
laplacianSVMclassifier
predictedPrice = predict(laplacianSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```

#### Bessel kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
besselSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="besseldot", cross = 10, scaled = TRUE)
besselSVMclassifier
predictedPrice = predict(besselSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```

</br>

###Training final SVM model
Based od root mean square error best model from all kernel and used data was SVM model with radial kernel, trained on data with column with many NA values. Therofore we now train this model on the whole train dataset. And use the 10-fold cross validation to see the RMSE on the whole dataset.
Note: We split the data in ration 1:1 because on when we used more data the ksvm crashed on error due to not enough memory
####Preparing data for final SVM model
```{r}
split = sample.split(data, SplitRatio = 0.5)
trainDf = subset(dataComplete, split == TRUE)
testDf = subset(dataComplete, split == FALSE)

trainDf <- trainDf[-c(9,11,14,16,17)]
testDf <- testDf[-c(9,11,14,16,17)]

traindDfWithoutPrice <- trainDf[-4]
testDfWithoutPrice <- testDf[-4]
```

###Training the model
```{r}
radialSVMclassifier <- ksvm(Price ~ ., trainDf, kernel="rbfdot", cross = 10, scaled = TRUE)
radialSVMclassifier
predictedPrice = predict(radialSVMclassifier, testDfWithoutPrice)
error = postResample(testDf$Price, predictedPrice)
error
```
