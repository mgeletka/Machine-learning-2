---
title: "assignment2"
date: "May 7, 2020"
output: html_document
---


##Introduction
The purpose of this assignment is t is to learn the main advantages and disadvantages of some machine learning regression methods by comparing:

* regression trees,
* bagging,
* random forest,
*  kernel approaches

</br>

##Data pre-processing

```{r, message=FALSE}
library(mice)
library(GGally)
library(geosphere)
library(dplyr)
library(VIM)
library(lubridate)
library(caTools)
library(e1071)
library(Metrics)
library(caret)



dataOriginal <- read.csv("Melbourne_housing_FULL.csv", header = TRUE, na.strings=c("", "NA", "#N/A"))

```


```{r, echo=FALSE}
# Fix date format
dataOriginal$Date<-as.Date(dataOriginal$Date, "%d/%m/%Y")
```

### NA values
Let's vizualise NA values
```{r}
aggr(dataOriginal,delimiter=NULL, plot=TRUE)
```

####Discard NA values of price
```{r}
dataOriginal <- dataOriginal[!is.na(dataOriginal$Price),]
```

####Replacing NA values

You can also embed plots, for example:

```{r}
#we don't need to do that apriori, but only if there will be any technique that doesn't work with NA 
set.seed(101) 
init = mice(dataOriginal, maxit=0) 
dataComplete <- complete(init)
```
</br>

###The target variable

####Distribution
```{r}
hist(dataOriginal$Price,col=4, main=paste("Price"), xlab="A$", breaks=seq(0,max(dataOriginal$Price),l=100))
```

#### Logarithmic distribution
```{r}
hist(log10(dataOriginal$Price),col=5, main=paste("Price"),xlab="log10(A$)", breaks=seq(log10(min(dataOriginal$Price)),log10(max(dataOriginal$Price)),l=100))
```

### Aggregating attributes and deriving new columns
```{r}

# Add year column and street column
addresses<-gsub("[[:punct:]]", "", dataComplete$Address)
addresses<-gsub("[[:digit:]]", "", addresses)

data<-mutate(dataComplete, Street=addresses, YearSaled=year(dataComplete$Date))

# Add season column
data<-mutate(data, Season=ifelse(month(data$Date) %in% 3:5, "Spring",ifelse(month(data$Date) %in% 6:8, "summer", ifelse(month(data$Date) %in% 9:11, "Autumn","Winter"))))

# Remove unuseful columns
data<-select(data,-c(Address,Date))

# Add a column with the distance to the center of the bay
MelbourneBayCentre <- cbind(144.783832, -38.104275)
data <- mutate(data, DistanceToBayCentre = distGeo(cbind(Longtitude, Lattitude), MelbourneBayCentre)/1000) #in km

# Add a column with the distance to the airports
MelbourneAirport<-c( 144.841802,-37.670149)
AvalonAirport<- c( 144.473152,-38.027377)
EssendonAirport<- c( 144.902035,-37.726376)
MoorabbinAirport <- c( 145.100306,-37.979454)
data<-mutate(data,DistanceToAirports = pmin(distGeo(cbind(Longtitude,Lattitude), MelbourneAirport), distGeo(cbind(Longtitude,Lattitude), AvalonAirport), distGeo(cbind(Longtitude,Lattitude), EssendonAirport), distGeo(cbind(Longtitude,Lattitude), MoorabbinAirport))/1000) #in km
```

</br>

#### Summary of data
```{r}
str(data)
summary(data)
```

</br>

####Spiltting the data
```{r}
set.seed(123)
data$Street <- factor(data$Street)
data$Season <- factor(data$Season)
split = sample.split(data, SplitRatio = 0.8)
traindDf = subset(data, split == TRUE)
testDf = subset(data, split == FALSE)
```

</br>

####Scalling values
```{r}
traindDfWithoutPrice <- traindDf[-4]
testDfWithoutPrice <- testDf[-4]

str(traindDf)
str(testDf)

# ind <- sapply(traindDfWithoutPrice, is.numeric)
# traindDfWithoutPrice[ind] <- lapply(traindDfWithoutPrice[ind], scale)
# 
# testDfWithoutPrice$Price <- scale(testDfWithoutPrice$Price)
# testDfWithoutPrice[ind] <- lapply(testDfWithoutPrice[ind], scale)
```

</br>

####Support Vector Machine (SVM)
```{r}
linearSVMclassifier <- svm(Price ~ ., traindDf, kernel="linear")
```

####Confusion Matrix of the classifier
```{r}
summary(linearSVMclassifier)
predictedPrice = predict(linearSVMclassifier, testDfWithoutPrice)

# summary(predictedPrice)
# length(testDf$Price)
postResample(testDf$Price, predictedPrice)

```


