---
title: "Machine Learning, 2019/2020 - Assignment 2"
author: "Martin Geletka, Alessandra Crippa, Giulio Rago"
date: "May 25, 2020"
output: html_document
---


## Introduction
The purpose of this assignment is to learn the main advantages and disadvantages of machine learning
regression methods, such as:

* regression
* regression trees,
* bagging,
* random forest,
*  kernel approaches

In this assignment, we will try to find the best hyperparameters for the given dataset. After finding the best models for each method
we compare all trained models by computed mean square error.

At the end of this assignment, we discuss the advantages and disadvantages of each method and choose the recommended approach for the given dataset.

```{r, warning=FALSE,message=False, echo=FALSE}
library(mice)
library(GGally)
library(geosphere)
library(dplyr)
library(VIM)
library(rpart) 
library(rattle) 
library(l2boost)
library(foreach)
library(e1071)  
library(caret)  
library(gbm)  
library(party) 
library(mboost) 
library(plyr) 
library(partykit) 
library(randomForest) 
library(lubridate)
library(caTools)
library(Metrics)
library(kernlab)
```

## Preprocessing the data
We start with reading the data from the CSV file and to better to work with the Date column we converted it to R Date type.
```{r, echo=FALSE}
dataOriginal <- read.csv("Melbourne_housing_FULL.csv", header = TRUE, na.strings=c("", "NA", "#N/A"))
dataOriginal$Date<-as.Date(dataOriginal$Date, "%d/%m/%Y")
```

### Summary of the data
To see the type of each column we call the str function on the original data. To see overview - mainly the range and number of NA vaues of each colum we called the summary. 

```{r, eval=FALSE}
  str(dataOriginal)
  summary(dataOriginal)
```

### NA values
Firstly to get a summary of the data, we visualize how a significant proportion of each column contains NA values. And then do it by all combinations of the columns. We see that the Landsize and YearBuilt have more than 50% of NA values. Other columns with considerable many NA values are Price, Bedroom2, Lattitude, Longitude.
```{r NAvizualisation}
aggr(dataOriginal, plot=TRUE)
```

####Discard NA values of price
Because we did not find any useful usage for the rows with NA values of the column Price, we removed them.
```{r}
dataOriginal <- dataOriginal[!is.na(dataOriginal$Price),]
```

####Replacing NA values
We also created data without NA values by using the package MICE. We use the default imputation methods, the pmm (predictive mean matching) for the numeric columns and polyreg(polytomous regression) for the factor columns, as shown in the summary. 

In each method, we will then decide if it is more suitable to use dataComplete or dataOriginal.


```{r dataComplete}
set.seed(101) 
init = mice(dataOriginal, maxit=0)
summary(init)
dataComplete <- complete(init)
```

</br>

### Exploring the target variable
Now we explore the distrution of the target varible Price. We visualize the histogram of this column in linear and also logarithmic scale and see that the its 
distrubion reminds the power-law. 

#### Linear Distribution

```{r histPrice}
hist(dataOriginal$Price,col=4, main=paste("Price"), xlab="Price")
```

#### Logarithmic distribution
```{r histlogPrice}
hist(log10(dataOriginal$Price),col=5, main=paste("Price"),xlab="Price")
```


</br>
  


### Aggregating attributes and deriving new columns
Here we created some additional columns which we think may be relevant to the Price prediction.
Created columns:
  * Street - from the column Adress we take an only string containing the street (then we used this column instead of the address)
  * Season - the season of the year taken from the column Data
  * YearSold - only the year of the column Date
  * MelbourneBayCentre - to approximate the distance to the beach we create the column of air distance to the middle of the Bay in Melbourne (location taken from Google Maps)
  * DistanceToAirports - to approximate the distance to the nearest airport we calculate minimal air-distance to the 4 Melbourne airports (location taken from Google Maps)


```{r new-columns}
aggregetaData <- function(data) {
  
  # Add year column and street column
  addresses<-gsub("[[:punct:]]", "", data$Address)
  addresses<-gsub("[[:digit:]]", "", addresses)
  
  newData<-mutate(data, YearSold=year(dataOriginal$Date))  
  
  # Add season column
  newData <- mutate(newData, Season=ifelse(month(newData$Date) %in% 3:5, "Spring",ifelse(month(newData$Date) %in% 6:8, "summer", ifelse(month(newData$Date) %in% 9:11, "Autumn","Winter"))))
  
  newData<-dplyr::select(newData,-c(Address,Date))
  
  # Add a column with the distance to the center of the bay
  MelbourneBayCentre <- cbind(144.783832, -38.104275)
  newData <- mutate(newData, DistanceToBayCentre = distGeo(cbind(Longtitude, Lattitude), MelbourneBayCentre)/1000)   #in km
  
  # Make from string variables factor
  newData$Season <- factor(newData$Season)
  
  # Add a column with the distance to the airports
  MelbourneAirport<-c( 144.841802,-37.670149)
  AvalonAirport<- c( 144.473152,-38.027377)
  EssendonAirport<- c( 144.902035,-37.726376)
  MoorabbinAirport <- c( 145.100306,-37.979454)
  newData<-mutate(newData,DistanceToAirports = pmin(distGeo(cbind(Longtitude,Lattitude), MelbourneAirport), distGeo(cbind(Longtitude,Lattitude), AvalonAirport), distGeo(cbind(Longtitude,Lattitude), EssendonAirport), distGeo(cbind(Longtitude,Lattitude), MoorabbinAirport))/1000) #in km
  
   return(newData)
}
data <- aggregetaData(dataOriginal)
dataComplete <- aggregetaData(dataComplete)
```


## Linear regression

### Create the train end test dataset
We firsly created smaller dataset for quick development and tuning of the hyperaparameters. We used dataComplete because the linear regression do not allow NA value in it's computations.
```{r}
split = sample.split(dataComplete, SplitRatio = 0.5)
smallDataset = subset(dataComplete, split == TRUE)
split = sample.split(data, SplitRatio = 0.7)
smallDatasetTrain = subset(smallDataset, split == TRUE)
smallDatasetTest = subset(smallDataset, split == FALSE)


```


```{r}
train.data <- sapply( smallDatasetTrain, as.numeric )
test.data <- sapply(smallDatasetTest, as.numeric)
smallDataset <- sapply(smallDataset, as.numeric)


smallDataset <- data.frame(smallDataset)
test.data <- data.frame(test.data)
train.data <- data.frame(train.data)
ytrain <- train.data[,"Price"]
ytest <- test.data[,"Price"] 
```

</br>

### Simple linear model
Firsly we created the simple linear model for predicting the column Price based on all other columns. After that we calculated R-squared, mean absolute error and root mean square error. Later on we compare the result from this simple model to see the effectss of the boosting and bagging.

```{r}

model <- lm(Price ~., data = train.data)

predictions <- model %>% predict(test.data)
f = data.frame(R2 = R2(predictions,test.data$Price),
           MAE = MAE(predictions, test.data$Price),
           RMSE = RMSE(predictions,test.data$Price))
summary(f)
```
### Cross validation
ALLESSANDRA  PLEASE EXPLAIN YOURSELF ??? TODO
```{r}
# Define training control
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model <- train(Price ~., data = smallDataset, method = "lm",trControl = train.control)
# Summarize the results
r2.cross = summary(model)$r.squared
```



### Bagging
Now we will see the effect of bagging on this simple linear model. Then we will compute the R-squared and root mean square error to see the effect of the bagging.
```{r}
length_divisor<-4
iterations<-2000
predictions<-foreach(m=1:iterations,.combine=cbind) %do% {
  training_positions <-  sample(nrow(train.data),size=floor((nrow(train.data)/length_divisor)))
  train_pos<-1:nrow(train.data) %in% training_positions
  lm_fit<-lm(Price~. ,data=train.data)
  predict(lm_fit,newdata=test.data)
}

predictions<-rowMeans(predictions)
error<-sqrt((sum((test.data$Price-predictions)^2)))/nrow(test.data)
difference <- predictions - test.data$Price
bagg.r2 = cor(test.data$Price,predictions)^2
bagg.rmse = sqrt(mean(difference^2))
print(bagg.r2)
print(bagg.rmse)
```


### Boosting
Now we will see the effect of boosting. Then we will plot the the histogram of the R-squared and root mean square error to see how they are distributed. ALLESSANDRA TODO WHY the plots?
```{r}
B = 2000
N = length(smallDataset[,1])

stor.r2 = rep(0,B)
stor.rmse = rep(0,B)

for(i in 1:B){
  idx = sample(1:N,N,replace = TRUE)
  newdata = smallDataset[idx,]
  newtestdata = test.data[idx,]
  model <- lm(Price~.,data = newtestdata)
  stor.r2[i] <- summary(model)$r.squared
  stor.rmse[i] <- sqrt(mean(model$residuals^2))
  
}

hist(stor.r2,col = "orange", main=paste("R-squared"),  xlab="R-squared")
hist(stor.rmse,col = "purple", main=paste("Root Mean Square Error"), xlab="Root Mean Square Error")

```

</br> 


###Comparing the effect of boosting and bagging
Now we compare the effect of boosting and bagging and by plotting the R-square and RMSE of each model.

```{r}
df<-data.frame(Methods=c('lm','bag','boost'),Rsquared=c(f$R2,bagg.r2,max(stor.r2)))
barplot(df$Rsquared,ylab='R-square',names.arg=df$Methods,
cex.names=2,col='blue',cex.axis=2,
ylim=c(0,0.7))

```

```{r}
df<-data.frame(Methods=c('lm','bag','boost'),Rsquared=c(f$RMSE,bagg.rmse,min(stor.rmse)))
barplot(df$Rsquared,ylab='RMSE',names.arg=df$Methods,
cex.names=2,col='red',cex.axis=2)

```

From these barplots we see that the model using boosting has best performance on this particular problem. The reason is that te boosting model has the highest R-squared value and lowest RMSE from all models.

</br>


## Regression trees
Since to compute cross validation on a rpart model we need to not have NA values, we proceed considering the dataset dataComplete.

#### Splitting the dataset into train and test
```{r train-test}
train.indexes <- sample(1:nrow(dataComplete),0.75*nrow(dataComplete))
data.train <- dataComplete[train.indexes,]
data.test <- dataComplete[-train.indexes,]
ytrain <- data.train[,"Price"]
ytest <- data.test[,"Price"] 
```


### Training the tree regression model with different cp value
We train the refression tree with differen value complexity parameter to find out the effect of this parameter on the final model.

```{r tree_different_cp}
cp_vect<-seq(0.001,0.3,(0.3-0.001)/30)
RMSE.tree.test=numeric(length(cp_vect))
RMSE.tree.train=numeric(length(cp_vect))
Rsquared.tree=numeric(length(cp_vect))
for (i in seq(1:length(cp_vect))){
  model.tree=rpart(Price ~ . ,data = data.train,cp=cp_vect[i]) 
  pred.tree.test <- predict(model.tree,newdata=data.test) 
  pred.tree.train <- predict(model.tree,newdata=data.train)
  RMSE.tree.test[i] <- sqrt(mean((pred.tree.test-ytest)^2))
  RMSE.tree.train[i] <- sqrt(mean((pred.tree.train-ytrain)^2))
  Rsquared.tree[i] <- 1 - sum((pred.tree.test-ytest)^2)/sum((ytest-mean(ytest))^2)
}
# plot RMSE vs cp
plot(cp_vect,RMSE.tree.test,type="l",col="red",xlab="cp",ylab="RMSE",main="RMSE")
lines(cp_vect,RMSE.tree.train,col="blue")
legend("bottomright",lty=1,col=c("red","blue"),legend = c("test ", "train "))
# plot Rsquared vs cp
plot(cp_vect,Rsquared.tree,type="l",col="blue", xlab="cp",ylab="R squared", main="R^2")
```

We can see that the smaller cp is, the better is the model (lower RMSE and higher Rsquared).


### Cross-validation
Let's apply 10-fold cross validation to confirm it.

We had to remove the columns of Suburbs and Sellers to run a later part of the code (bagging and boosting), so we remove those columns from here in order to make a comparison between the different techniques. 
```{r CVtree_tuning_cp, warning=FALSE}
train_control <- trainControl(method="cv", number=10)
evaltree <- train(Price~., data=dataComplete[,-c(1,6)], trControl=train_control, method="rpart",tuneLength = 10)
evaltree$bestTune
```

### Training the tree regression model with different max.depth values

Now we want to see how different values of max.depth affect our tree regression model. We also plot the of RMSE an R-squared to see the effect of the parameter of max.depth to the model.
```{r tree_different_depth}
# learn a tree regression model with different values of max.depth, and the best cp value found before
depth_vect <- seq (1,30,1)
RMSE.tree.test=numeric(length(depth_vect))
RMSE.tree.train=numeric(length(depth_vect))
Rsquared.tree=numeric(length(depth_vect))
for (i in seq(1:length(depth_vect))){
  model.tree=rpart(Price ~ . ,data = data.train,maxdepth=depth_vect[i],cp=evaltree$bestTune) 
  pred.tree.test <- predict(model.tree,newdata=data.test) 
  pred.tree.train <- predict(model.tree,newdata=data.train)
  RMSE.tree.test[i] <- sqrt(mean((pred.tree.test-ytest)^2))
  RMSE.tree.train[i] <- sqrt(mean((pred.tree.train-ytrain)^2))
  Rsquared.tree[i] <- 1 - sum((pred.tree.test-ytest)^2)/sum((ytest-mean(ytest))^2)
}
```

```{r}
# plot RMSE
plot(depth_vect,RMSE.tree.test,type="l",col="red",xlab="cp",ylab="RMSE",main="RMSE", ylim=c(350000, 575000))
lines(depth_vect,RMSE.tree.train,col="blue")
legend("bottomright",lty=1,col=c("red","blue"),legend = c("test ", "train "))
```

```{r}
# plot Rsquared
plot(depth_vect,Rsquared.tree,type="l",col="blue", xlab="cp",ylab="R squared", main="R^2")
```

We can see that for a depth greater than 5 RMSE and R squared converge. Because of that we decided to keep the default value max.depth=30.

### Final standard tree regression model
</br>
This is the final model obtained through cross validation on the tuning parameter cp.
```{r}
fancyRpartPlot(evaltree$finalModel)
#RMSE and Rsquared of our final model
rmse1 <- evaltree$results$RMSE[1]
rs1 <- evaltree$results$Rsquared[1]
```

</br>

Now we want to see if we can improve the model with bagging and boosting.


### Bagging
We tried the effect of bagging and computed the RMSE and R-squared to compare with other models.
```{r bagging_tree}
evaltreebag <- train(Price~., data=dataComplete[,-c(1,6)], trControl=train_control, method="treebag") 
# R squared
rs2<-evaltreebag$results$Rsquared
# RMSE
rmse2<-evaltreebag$results$RMSE
```

### Boosting
We tried the effect of boosting and computed the RMSE and R-squared to compare with other models.
```{r boosting_tree_pb_solution, echo=FALSE}
#problem risolution
councarea<-lapply(dataComplete$CouncilArea, as.character)
councarea<-gsub(pattern = "\\s",   
     replacement = "",
     x = councarea)
dataComplete$CouncilArea<-as.factor(councarea)
regionnames<-lapply(dataComplete$Regionname, as.character)
regionnames<-gsub(pattern = "\\s",   
     replacement = "",
     x = regionnames)
regionnames<-gsub('-',"",regionnames)
regionnames<-as.factor(regionnames)
dataComplete$Regionname<-regionnames
```

```{r boosting_tree}
evaltreeboost <- train(Price~., data=dataComplete[,-c(1,6)], trControl=train_control, method="blackboost") #takes 35/40 minutes to run D:
# R squared
rs3<-evaltreeboost$results$Rsquared[9]
# RMSE
rmse3<-evaltreeboost$results$RMSE[9]
```

```{r}
# Let's see with Cross Validation what are the best tuning paramenters for boosting
plot(evaltreeboost)
evaltreeboost$bestTune
```

### Final comparison
Now we can compare regression trees, bagged regression trees, boosted regression trees, based on the computed values of R-squared and RMSE.
```{r comparison_tree_models}
df<-data.frame(Methods=c('rpart','bag tree','boost tree'),Rsquared=c(rs1,rs2,rs3))
barplot(df$Rsquared,ylab='R-square',names.arg=df$Methods,
cex.names=2,col='blue',cex.axis=2,
ylim=c(0,1))
df<-data.frame(Methods=c('rpart','bag tree','boost tree'),Rsquared=c(rmse1,rmse2,rmse3))
barplot(df$Rsquared,ylab='RMSE',names.arg=df$Methods,
cex.names=2,col='blue',cex.axis=2)
```

We can see from the barplot that the boosted model is the best one, with the highest R squared and the smallest RMSE.

</br>

## Support Vector Machine (SVM)

</br>

#### Small dataset
We created smaller sample of the data in purpose of the tuning of the hyperparameter of the model. Otherwise finding these parameters would took too much time, because of the high computation time, which take SVM fitting fuction. 
```{r small_dataset}
set.seed(123)
split = sample.split(dataComplete, SplitRatio = 0.1)
smallDataset = subset(dataComplete, split == TRUE)
split = sample.split(data, SplitRatio = 0.7)
smallDatasetTrain = subset(smallDataset, split == TRUE)
smallDatasetTest = subset(smallDataset, split == FALSE)
traindDfWithoutPrice <- smallDatasetTrain[-4]
testDfWithoutPrice <- smallDatasetTest[-4]
```

</br>



Although the method ksvm does not fail when given NA value, internally is omitting them and not learning anything from them. So with SVM, we used the complete dataset created by data imputation.

### {.tabset .tabset-fade .tabset-pills}
Now we tried a couple of SVM with a different model on all columns of the data. We perform 10-fold cross-validation to see the training error and cross-validation error of each model.

#### Linear kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}

```{r, message=FALSE}
linearSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="vanilladot", cross = 10, scaled = TRUE)
linearSVMclassifier
predictedPrice = predict(linearSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```

#### Anova kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r, message=FALSE}
anovaSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="anovadot", cross = 10, scaled = TRUE)
anovaSVMclassifier
predictedPrice = predict(anovaSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Radial Basis kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
radialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="rbfdot", cross = 10, scaled = TRUE)
radialSVMclassifier
predictedPrice = predict(radialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 3 the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=3), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 5 {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=5), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 8 {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=8), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Laplacian kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
laplacianSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="laplacedot", cross = 10, scaled = TRUE)
laplacianSVMclassifier
predictedPrice = predict(laplacianSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```

#### Bessel kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r message=FALSE}
besselSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="besseldot", cross = 10, scaled = TRUE)
besselSVMclassifier
predictedPrice = predict(besselSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```

</br>

### Removing columns with too much NA values in original data
### {.tabset .tabset-fade .tabset-pills}
Now we try to remove columns which have many NA values. So we remove the columns Bedroom2, Car, YearBuild and lattitude and longtitide. Then we perform 10-fold cross validation to see the training error and cross validation error of each model on those reduced dataset.
```{r}
smallDatasetTrain <- smallDatasetTrain[-c(9,11,14,16,17)]
smallDatasetTest <- smallDatasetTest[-c(9,11,14,16,17)]
```

#### Linear kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r message=FALSE}
linearSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="vanilladot", cross = 10, scaled = TRUE)
linearSVMclassifier
predictedPrice = predict(linearSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```

#### Anova kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r  message=FALSE}
anovaSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="anovadot", cross = 10, scaled = TRUE)
anovaSVMclassifier
predictedPrice = predict(anovaSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Radial Basis kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
radialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="rbfdot", cross = 10, scaled = TRUE)
radialSVMclassifier
predictedPrice = predict(radialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 3 the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=3), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 5 {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=5), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Polynomial kernel classifier of degree 8 {.tabset .tabset-fade .tabset-pills}
```{r}
polynomialSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="polydot", kpar = list(degree=8), cross = 10, scaled = TRUE)
polynomialSVMclassifier
predictedPrice = predict(polynomialSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```
#### Laplacian kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}
```{r}
laplacianSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="laplacedot", cross = 10, scaled = TRUE)
laplacianSVMclassifier
predictedPrice = predict(laplacianSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```

#### Bessel kernel classifier of the classifier {.tabset .tabset-fade .tabset-pills}

```{r message=FALSE}
besselSVMclassifier <- ksvm(Price ~ ., smallDatasetTrain, kernel="besseldot", cross = 10, scaled = TRUE)
besselSVMclassifier
predictedPrice = predict(besselSVMclassifier, testDfWithoutPrice)
error = postResample(smallDatasetTest$Price, predictedPrice)
error
```


</br>


### Training final SVM model

Based on root mean square error best model from all kernel and used data was SVM model with radial kernel, trained on data without columns with many NA values. Therefore we now train this model on the whole train dataset. And use the 10-fold cross validation to see the RMSE on the whole dataset.

Note: We split the data in ration 1:1 because on when we used more data the ksvm crashed on error due to not enough memory
#### Preparing data for final SVM model
```{r}
split = sample.split(data, SplitRatio = 0.5)
trainDf = subset(dataComplete, split == TRUE)
testDf = subset(dataComplete, split == FALSE)
trainDf <- trainDf[-c(9,11,14,16,17)]
testDf <- testDf[-c(9,11,14,16,17)]
traindDfWithoutPrice <- trainDf[-4]
testDfWithoutPrice <- testDf[-4]
```


#### Training the model
```{r final_SVM_model}
radialSVMclassifier <- ksvm(Price ~ ., trainDf, kernel="rbfdot", cross = 10, scaled = TRUE)
radialSVMclassifier
predictedPrice = predict(radialSVMclassifier, testDfWithoutPrice)
error = postResample(testDf$Price, predictedPrice)
error
```
</br>


## Comparing SVM with ensemble approaches
After testing the different techniques seen in the classroom we can see some aspects:
in the two ensemble approaches (linear regression and regression tree)
the best model is obtained by using the boosting technique.
Between the two ensemble method as we can expect the regression tree obtain a better result, this outcome is for the nature of the method, the regression tree works particularly well with a dataset of these amount of variables and features.
The biggest difference between the two class of method is that SVM uses the kernel trick to turn a linearly nonseparable problem into a linearly separable one, while ensemble methods split the input space into hyper-rectangles according to the target.


## Final recommended approach
Now we can compare the obtained values of R-squared and RMSE of the best developed models from each techniques. For visualize the particaluar values of each model in barplot and compare. We see that the Tree approach and SVM model has comparable results. But the SVM approach is slightly better and therefore recommenced by us for this particular problem. 
```{r}
df<-data.frame(Methods=c('Linear mod.','Trees','SVM'),Rsquared=c(0.5424989, 7.24531e-01,7.344281e-01))
barplot(df$Rsquared,ylab='R-square',names.arg=df$Methods,
cex.names=2,col='blue',cex.axis=2,
ylim=c(0,1))

df<-data.frame(Methods=c('Linear mod.','Trees','SVM'), RMSE=c(447438, 3.586482e+05, 3.375243e+05))
barplot(df$RMSE,ylab='RMSE',names.arg=df$Methods,
cex.names=2,col='blue',cex.axis=2)
```

</br>

## Conclusion
In this assignment, we get in touch with different techniques for dealing with regression problems. Firstly we learned and came up with some methods of how to aggregate spatial data, like computing the distances for some significant points. Then we saw the effect of bagging and boosting both on regression trees and linear models. Then we saw that the finding of the best kernel function for the SVM method it's not always easy and does not need to be intuitive for the first time and this can be one of the biggest disadvantages of the SVM approach. We also saw that this choice of the kernel function has a significant impact on the performance of the final model and therefore we should compare different .  And finally, we came up with the recommended approach by comparing the obtained R-squared and RMSE of each computed model.
